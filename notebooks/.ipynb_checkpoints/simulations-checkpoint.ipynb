{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Criticism for Bayesian Causal Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a class=\"anchor\" id=\"imports21\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/lguelman/Library/Mobile Documents/com~apple~CloudDocs/LG_Files/Development/MCBCI/python')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "parameters = {'figure.figsize': (8, 4),\n",
    "              'font.size': 6, \n",
    "              'axes.labelsize': 10}\n",
    "plt.rcParams.update(parameters)\n",
    "plt.style.use('fivethirtyeight')\n",
    "from IPython.display import Image\n",
    "\n",
    "import pystan\n",
    "import multiprocessing\n",
    "import stan_utility\n",
    "import arviz as az\n",
    "\n",
    "import seaborn as sns\n",
    "from utils import stan_model_summary, DataGeneratingProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Syntehic Data \n",
    "\n",
    "We generate synthetic data according to Section 4.1 of the [paper](https://arxiv.org/abs/1610.09037) with minor variants. Specifically, we generate 500 data points, each with a 10-dimensional covariate $x_i$, a binary treatment $a_i$, and a set of potential outcomes $(y_i(0), y_i(1))$,\n",
    "\n",
    "\\begin{align*} \n",
    "x_i &\\sim  \\text{Uniform}\\big(x_i~\\vert~[0,1]^{10} \\big), \\\\ \n",
    "a_i~\\vert~x_i &\\sim \\text{Bernoulli} \\big(a_i~\\vert~ \\text{logistic}(x_i^\\intercal \\phi)  \\big) \\\\\n",
    "y_i(a)~\\vert~x_i &\\sim \\mathscr{N} \\big(y_i(a)~\\vert~[x_i,a]^\\intercal \\theta, \\sigma^2  \\big) \\\\\n",
    "\\phi, \\theta &\\sim \\mathscr{N}(0,1) \\\\\n",
    "\\sigma^2 &\\sim \\text{Gamma}(1,1) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We further assume zero correlation $\\rho$ between potential outcome. This can be modified by changing  the parameter `rho` in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_obs</th>\n",
       "      <th>y_mis</th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>tau_unit</th>\n",
       "      <th>a</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.355080</td>\n",
       "      <td>1.924043</td>\n",
       "      <td>-0.355080</td>\n",
       "      <td>1.924043</td>\n",
       "      <td>2.279124</td>\n",
       "      <td>0</td>\n",
       "      <td>0.903661</td>\n",
       "      <td>0.896010</td>\n",
       "      <td>0.589950</td>\n",
       "      <td>0.987273</td>\n",
       "      <td>0.851603</td>\n",
       "      <td>0.208817</td>\n",
       "      <td>0.766623</td>\n",
       "      <td>0.032806</td>\n",
       "      <td>0.296746</td>\n",
       "      <td>0.570360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.246395</td>\n",
       "      <td>-0.049397</td>\n",
       "      <td>-0.049397</td>\n",
       "      <td>2.246395</td>\n",
       "      <td>2.295791</td>\n",
       "      <td>1</td>\n",
       "      <td>0.834669</td>\n",
       "      <td>0.160730</td>\n",
       "      <td>0.348411</td>\n",
       "      <td>0.030891</td>\n",
       "      <td>0.986353</td>\n",
       "      <td>0.501477</td>\n",
       "      <td>0.437794</td>\n",
       "      <td>0.652700</td>\n",
       "      <td>0.470989</td>\n",
       "      <td>0.948481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.486919</td>\n",
       "      <td>4.878609</td>\n",
       "      <td>2.486919</td>\n",
       "      <td>4.878609</td>\n",
       "      <td>2.391690</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587839</td>\n",
       "      <td>0.020352</td>\n",
       "      <td>0.817677</td>\n",
       "      <td>0.977189</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>0.849895</td>\n",
       "      <td>0.906638</td>\n",
       "      <td>0.699583</td>\n",
       "      <td>0.717297</td>\n",
       "      <td>0.576994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.665241</td>\n",
       "      <td>1.605872</td>\n",
       "      <td>1.605872</td>\n",
       "      <td>3.665241</td>\n",
       "      <td>2.059369</td>\n",
       "      <td>1</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.246183</td>\n",
       "      <td>0.523785</td>\n",
       "      <td>0.471349</td>\n",
       "      <td>0.020301</td>\n",
       "      <td>0.634659</td>\n",
       "      <td>0.927902</td>\n",
       "      <td>0.888842</td>\n",
       "      <td>0.703946</td>\n",
       "      <td>0.204001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.151500</td>\n",
       "      <td>1.480845</td>\n",
       "      <td>-0.151500</td>\n",
       "      <td>1.480845</td>\n",
       "      <td>1.632345</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836528</td>\n",
       "      <td>0.774359</td>\n",
       "      <td>0.919492</td>\n",
       "      <td>0.569234</td>\n",
       "      <td>0.779445</td>\n",
       "      <td>0.185953</td>\n",
       "      <td>0.868888</td>\n",
       "      <td>0.660685</td>\n",
       "      <td>0.276374</td>\n",
       "      <td>0.231046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.011190</td>\n",
       "      <td>2.039205</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>2.039205</td>\n",
       "      <td>2.028015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.621539</td>\n",
       "      <td>0.915605</td>\n",
       "      <td>0.606129</td>\n",
       "      <td>0.818001</td>\n",
       "      <td>0.221810</td>\n",
       "      <td>0.684723</td>\n",
       "      <td>0.639255</td>\n",
       "      <td>0.157652</td>\n",
       "      <td>0.979599</td>\n",
       "      <td>0.064534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>3.286602</td>\n",
       "      <td>0.806697</td>\n",
       "      <td>0.806697</td>\n",
       "      <td>3.286602</td>\n",
       "      <td>2.479904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593499</td>\n",
       "      <td>0.713316</td>\n",
       "      <td>0.609092</td>\n",
       "      <td>0.450044</td>\n",
       "      <td>0.069620</td>\n",
       "      <td>0.503841</td>\n",
       "      <td>0.571416</td>\n",
       "      <td>0.575057</td>\n",
       "      <td>0.662503</td>\n",
       "      <td>0.689368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1.815444</td>\n",
       "      <td>3.980675</td>\n",
       "      <td>1.815444</td>\n",
       "      <td>3.980675</td>\n",
       "      <td>2.165231</td>\n",
       "      <td>0</td>\n",
       "      <td>0.316736</td>\n",
       "      <td>0.763296</td>\n",
       "      <td>0.181927</td>\n",
       "      <td>0.506575</td>\n",
       "      <td>0.361373</td>\n",
       "      <td>0.995647</td>\n",
       "      <td>0.226801</td>\n",
       "      <td>0.827097</td>\n",
       "      <td>0.517792</td>\n",
       "      <td>0.622547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>3.007869</td>\n",
       "      <td>0.867875</td>\n",
       "      <td>0.867875</td>\n",
       "      <td>3.007869</td>\n",
       "      <td>2.139993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.064120</td>\n",
       "      <td>0.253654</td>\n",
       "      <td>0.339305</td>\n",
       "      <td>0.258092</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.290471</td>\n",
       "      <td>0.342010</td>\n",
       "      <td>0.493644</td>\n",
       "      <td>0.510703</td>\n",
       "      <td>0.632596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1.816440</td>\n",
       "      <td>3.778707</td>\n",
       "      <td>1.816440</td>\n",
       "      <td>3.778707</td>\n",
       "      <td>1.962267</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012971</td>\n",
       "      <td>0.279319</td>\n",
       "      <td>0.578296</td>\n",
       "      <td>0.684492</td>\n",
       "      <td>0.196573</td>\n",
       "      <td>0.738876</td>\n",
       "      <td>0.918291</td>\n",
       "      <td>0.741844</td>\n",
       "      <td>0.254321</td>\n",
       "      <td>0.140409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_obs     y_mis        y0        y1  tau_unit  a        X1        X2  \\\n",
       "0   -0.355080  1.924043 -0.355080  1.924043  2.279124  0  0.903661  0.896010   \n",
       "1    2.246395 -0.049397 -0.049397  2.246395  2.295791  1  0.834669  0.160730   \n",
       "2    2.486919  4.878609  2.486919  4.878609  2.391690  0  0.587839  0.020352   \n",
       "3    3.665241  1.605872  1.605872  3.665241  2.059369  1  0.398804  0.246183   \n",
       "4   -0.151500  1.480845 -0.151500  1.480845  1.632345  0  0.836528  0.774359   \n",
       "..        ...       ...       ...       ...       ... ..       ...       ...   \n",
       "495  0.011190  2.039205  0.011190  2.039205  2.028015  0  0.621539  0.915605   \n",
       "496  3.286602  0.806697  0.806697  3.286602  2.479904  1  0.593499  0.713316   \n",
       "497  1.815444  3.980675  1.815444  3.980675  2.165231  0  0.316736  0.763296   \n",
       "498  3.007869  0.867875  0.867875  3.007869  2.139993  1  0.064120  0.253654   \n",
       "499  1.816440  3.778707  1.816440  3.778707  1.962267  0  0.012971  0.279319   \n",
       "\n",
       "           X3        X4        X5        X6        X7        X8        X9  \\\n",
       "0    0.589950  0.987273  0.851603  0.208817  0.766623  0.032806  0.296746   \n",
       "1    0.348411  0.030891  0.986353  0.501477  0.437794  0.652700  0.470989   \n",
       "2    0.817677  0.977189  0.177341  0.849895  0.906638  0.699583  0.717297   \n",
       "3    0.523785  0.471349  0.020301  0.634659  0.927902  0.888842  0.703946   \n",
       "4    0.919492  0.569234  0.779445  0.185953  0.868888  0.660685  0.276374   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "495  0.606129  0.818001  0.221810  0.684723  0.639255  0.157652  0.979599   \n",
       "496  0.609092  0.450044  0.069620  0.503841  0.571416  0.575057  0.662503   \n",
       "497  0.181927  0.506575  0.361373  0.995647  0.226801  0.827097  0.517792   \n",
       "498  0.339305  0.258092  0.032000  0.290471  0.342010  0.493644  0.510703   \n",
       "499  0.578296  0.684492  0.196573  0.738876  0.918291  0.741844  0.254321   \n",
       "\n",
       "          X10  \n",
       "0    0.570360  \n",
       "1    0.948481  \n",
       "2    0.576994  \n",
       "3    0.204001  \n",
       "4    0.231046  \n",
       "..        ...  \n",
       "495  0.064534  \n",
       "496  0.689368  \n",
       "497  0.622547  \n",
       "498  0.632596  \n",
       "499  0.140409  \n",
       "\n",
       "[500 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgp = DataGeneratingProcess(N=500, d=10, rho=0.0, seed=432)\n",
    "data = dgp.generate_data()\n",
    "data.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model-Based Inference \n",
    "\n",
    "We approximate the posterior with MCMC in [Stan](https://mc-stan.org/) under three alternative causal models. We obtain 2000 samples from the posterior of all variables in the causal model: $y(0),~y(1),~\\mathbf{a},~\\phi,~\\theta,~ \\text{and}~\\sigma^2$. The correlation between potential outcomes, $\\rho$, is the only parameter about which the observed data cannot provide empirical information becuase $y_i(0)$ and $y_i(1)$ are never observed simultaneously. Thus, model-based inference requires subject-matter knowledge and sensible assumptions on the joint distribution of potential outcomes.\n",
    "\n",
    "There are two sources of uncertainty in the posterior predictive distribution of the missing potential outcomes $P(y^{\\text{miss}} | y^{\\text{obs}}, a)$. The first is the uncertainty in the estimated latent variables $\\theta, ~\\sigma^2$ (a.k.a., *epistemic uncertainty*). The second is the uncertainty in the data as expressed by the Gaussian random sampling mechanism (a.k.a., *aleatoric uncertainty*). The same holds for the posterior predictive distribution of the treatment assignment, with epistemic uncertainty being determined by the posterior distribution $P(\\phi|x, y(\\mathbf{a}), \\mathbf{a})$, and aleatoric uncertainty determined by the Bernoulli random sampling process $P(\\mathbf{a}|x, \\phi)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Case A: The Correct Model\n",
    "\n",
    "Assume we implement a model that is faithful to the specification outlined in Section 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_c8b76d24602c7f85b15751b502f10940 NOW.\n"
     ]
    }
   ],
   "source": [
    "stan_data_dict = {'N': data.N,       # sample size\n",
    "                  'd_a': data.d,     # number of covariates in the assignment model\n",
    "                  'd_o': data.d,     # number of covariates in the outcome model\n",
    "                  'X_a': data.X,     # covariates matrix for assignment model\n",
    "                  'X_o': data.X,     # covariates matrix for outcome model  \n",
    "                  'y': data.y_obs,   # observed outcome\n",
    "                  'a': data.a,       # treatment assigned\n",
    "                  'rho': 0.0,        # correlation between the potential outcomes (assumed)\n",
    "                  'a_miss':0,        # assignment model misspecified (1), or not (0). \n",
    "                  }\n",
    "\n",
    "sm = pystan.StanModel('../stan/mbi_stan.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit_a = sm.sampling(data=stan_data_dict, iter=1000, chains=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Case B: Misspecified Outcome Model\n",
    "\n",
    "Assume we misspecify the distribution over $y_i(a)$ by ignoring the first entry on $x_i$ (i.e., $\\theta_1=0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data_dict_b = {'N': data.N,\n",
    "                    'd_a': data.d,\n",
    "                    'd_o': data.d-1,\n",
    "                    'X_a': data.X,\n",
    "                    'X_o': data.X[:,1:10],\n",
    "                    'y': data.y_obs,\n",
    "                    'a': data.a,\n",
    "                    'rho': 0.0,\n",
    "                    'a_miss':0,\n",
    "                   }\n",
    "\n",
    "sm_b = pystan.StanModel('../stan/mbi_stan.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit_b = sm_b.sampling(data=stan_data_dict_b, iter=1000, chains=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Case C: Misspecified Assignment Model\n",
    "\n",
    "Assume we misspecify the distribution over $a_i$ by setting the probability that $a_i=1$ to: $~0.7+0.3 \\times \\text{logistic}(x_i^\\intercal \\phi)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data_dict_c = {'N': data.N,\n",
    "                    'd_a': data.d,\n",
    "                    'd_o': data.d,\n",
    "                    'X_a': data.X,\n",
    "                    'X_o': data.X,\n",
    "                    'y': data.y_obs,\n",
    "                    'a': data.a,\n",
    "                    'rho': 0.0,\n",
    "                    'a_miss':1,\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_c = pystan.StanModel('../stan/mbi_stan.stan') \n",
    "multiprocessing.set_start_method(\"fork\", force=True)\n",
    "fit_c = sm_c.sampling(data=stan_data_dict_c, iter=1000, chains=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Criticism \n",
    "\n",
    "Model criticism measures the degree to which a model falsely describes the data. The central tool for this is the posterior predictive check (PPC). It quantifies the degree to which data generated from the model deviate from the observe data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Criticizing the Assignment Model\n",
    "\n",
    "We plot below the *reference discrepancy* vs. *realized discrepancy*. We've used mean probability of assignment instead of log-likelihood (used in the paper) as the discrepancy function.\n",
    "\n",
    "The test for the correct assignment model does not suggest any issue, but with the \"wrong\"  model, the test fails. This correctly indicates that we should revise the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_a = fit_a.extract(permuted=True)\n",
    "\n",
    "#p_phi_a = samples_a['phi']\n",
    "p_a_rep_a = samples_a['a_rep']\n",
    "\n",
    "p = sns.histplot(data=np.mean(p_a_rep_a, axis=1), stat=\"density\", label=\"Reference\") \n",
    "p.set(xlabel='Average prob of assignment', title='Correct Specification')\n",
    "plt.axvline(np.quantile(np.mean(p_a_rep_a, axis=1), .1), color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(np.mean(p_a_rep_a, axis=1), .9), color='g', linestyle='--')\n",
    "plt.axvline(np.mean(data.a), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_c = fit_c.extract(permuted=True)\n",
    "\n",
    "#p_phi_c = samples_c['phi']\n",
    "p_a_rep_c = samples_c['a_rep']\n",
    "\n",
    "p = sns.histplot(data=np.mean(p_a_rep_c, axis=1), stat=\"density\", label=\"Reference\")\n",
    "p.set(xlabel='Average prob of assignment', title='Wrong Assignment Model')\n",
    "plt.axvline(np.quantile(np.mean(p_a_rep_c, axis=1), .1), color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(np.mean(p_a_rep_c, axis=1), .9), color='g', linestyle='--')\n",
    "plt.axvline(np.mean(data.a), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Criticizing the Outcome Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. \"Science-Fiction\" scenario: Assume access to counterfactual outcomes\n",
    "\n",
    "We display the reference distribution for the average unit-level treatment effect computed from samples from the posterior predictive distribution of potential outcomes. The realized discrepancy is given by the actual average treatment (which is not accessible in real-world scenario).\n",
    "\n",
    "Again, as expected the test fails for the case in which the outcome model is misspecified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference distribution for unit-level treatment effect\n",
    "p = sns.histplot(data=np.mean(samples_a['tau_unit'], axis=1), stat=\"density\", \n",
    "                 label=\"Reference\")\n",
    "p.set(xlabel='Average unit-level treatment effect', title='Correct Specification')\n",
    "plt.axvline(np.quantile(np.mean(samples_a['tau_unit'], axis=1), .1), \n",
    "            color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(np.mean(samples_a['tau_unit'], axis=1), .9),\n",
    "            color='g', linestyle='--')\n",
    "\n",
    "# Realized discrepancy for unit-level treatment effect\n",
    "plt.axvline(np.mean(data.df.tau_unit.values), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_b = fit_b.extract(permuted=True) # misspecified outcome model\n",
    "#Reference distribution for unit-level treatment effect\n",
    "p = sns.histplot(data=np.mean(samples_b['tau_unit'], axis=1), stat=\"density\", label=\"Reference\")\n",
    "p.set(xlabel='Average unit-level treatment effect', title='Wrong Outcome Model')\n",
    "plt.axvline(np.quantile(np.mean(samples_b['tau_unit'], axis=1), .1), color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(np.mean(samples_b['tau_unit'], axis=1), .9), color='g', linestyle='--')\n",
    "# Realized discrepancy for unit-level treatment effect\n",
    "plt.axvline(np.mean(data.df.tau_unit.values), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. \"Fiction\" scenario: Assume access to factual outcomes only\n",
    "\n",
    "The test does not fail for the model in which the outcome model is misspecified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference distribution - Average log-likelihood of potential outcomes\n",
    "\n",
    "ref_dist = np.mean(samples_a['log_lik_ref_y0'].T + \n",
    "                   samples_a['log_lik_ref_y1'].T, axis=0)\n",
    "p = sns.histplot(data=ref_dist, stat=\"density\", label=\"Reference\")\n",
    "p.set(xlabel='Average log-likelihood', title='Correct Specification')\n",
    "plt.axvline(np.quantile(ref_dist, .1), color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(ref_dist, .9), color='g', linestyle='--')\n",
    "\n",
    "# Realized discrepancy\n",
    "pi = samples_a['a_prob_rep'].T \n",
    "weighted_log_lik_y_obs = np.zeros(shape=(samples_a['log_lik_y_obs'].T.shape))\n",
    "for i in range(len(data.a)):\n",
    "    if data.a[i] == 0:\n",
    "        weighted_log_lik_y_obs[i, :]=samples_a['log_lik_y_obs'].T[i,:]/(1-pi[i,:])\n",
    "    else:\n",
    "        weighted_log_lik_y_obs[i, :]=samples_a['log_lik_y_obs'].T[i,:]/(pi[i,:])\n",
    "plt.axvline(np.mean(weighted_log_lik_y_obs), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference distribution - Average log-likelihood of potential outcomes\n",
    "\n",
    "ref_dist = np.mean(samples_b['log_lik_ref_y0'].T + \n",
    "                   samples_b['log_lik_ref_y1'].T, axis=0)\n",
    "p = sns.histplot(data=ref_dist, stat=\"density\", label=\"Reference\")\n",
    "p.set(xlabel='Average log-likelihood', title='Wrong Specification')\n",
    "plt.axvline(np.quantile(ref_dist, .1), color='g', linestyle='--', label='90% intervals')\n",
    "plt.axvline(np.quantile(ref_dist, .9), color='g', linestyle='--')\n",
    "\n",
    "# Realized discrepancy \n",
    "pi = samples_b['a_prob_rep'].T \n",
    "weighted_log_lik_y_obs = np.zeros(shape=(samples_b['log_lik_y_obs'].T.shape))\n",
    "for i in range(len(data.a)):\n",
    "    if data.a[i] == 0:\n",
    "        weighted_log_lik_y_obs[i, :]=samples_b['log_lik_y_obs'].T[i,:]/(1-pi[i,:])\n",
    "    else:\n",
    "        weighted_log_lik_y_obs[i, :]=samples_b['log_lik_y_obs'].T[i,:]/(pi[i,:])\n",
    "plt.axvline(np.mean(weighted_log_lik_y_obs), color='r', label='Realized')\n",
    "plt.legend(loc=\"upper right\", fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
